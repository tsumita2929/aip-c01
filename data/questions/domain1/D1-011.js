window.DOMAIN1_QUESTIONS = window.DOMAIN1_QUESTIONS || [];
window.DOMAIN1_QUESTIONS.push({
  "id": "D1-011",
  "domain": 1,
  "task": "1.2",
  "skill": "1.2.1",
  "type": "single",
  "difficulty": "medium",
  "question": "ある保険会社が、カスタマーサポート用のチャットボットを Amazon Bedrock で構築しています。チャットボットは日本語で保険約款に関する質問に回答する必要があり、応答レイテンシーは3秒以内、月間コストは $3,000 以内という制約があります。候補として Anthropic Claude 3.5 Sonnet、Anthropic Claude 3 Haiku、Amazon Titan Text Express の3モデルが挙がっています。最も適切なモデル選定アプローチはどれですか？",
  "options": [
    {
      "id": "A",
      "text": "Bedrock のモデル評価ジョブを作成し、自社の保険約款Q&Aテストデータセット（200問）を使用して各モデルの正確性（ROUGE スコア）を測定する。同時に CloudWatch メトリクスでレイテンシーを計測し、オンデマンド料金表から月間コストを試算して総合的に比較する"
    },
    {
      "id": "B",
      "text": "Bedrock プレイグラウンドで各モデルに10件の代表的な質問を入力し、応答の日本語品質を確認する。最も自然な日本語を生成するモデルを選定し、API Gateway + Lambda 経由で Bedrock の Converse API を呼び出す構成で本番デプロイする"
    },
    {
      "id": "C",
      "text": "Bedrock のモデル評価ジョブで人間による評価ワークフローを設定し、保険部門の社員20名に正確性・関連性・有用性を5段階でスコアリングさせる。最高スコアのモデルを選定する"
    },
    {
      "id": "D",
      "text": "各モデルの公式ベンチマーク結果（MMLU、HellaSwag等）を比較し、総合スコアが最も高いモデルを選定する。選定後に Bedrock のプロンプト管理でプロンプトテンプレートを作成し、保険約款に特化したシステムプロンプトを設定する"
    }
  ],
  "correctAnswers": [
    "A"
  ],
  "explanation": "モデル評価ジョブで自社の保険約款Q&Aテストデータを使用した定量評価と、CloudWatch メトリクスによるレイテンシー計測、料金表からのコスト試算を組み合わせることで、日本語対応・レイテンシー・コストの3要件すべてを客観的に評価できます。プレイグラウンドでの10件テスト（B）は初期の感触確認には有用ですが、10件では統計的有意性が不十分であり、特に保険約款の多様な質問パターンをカバーできません。レイテンシーやコストの体系的な比較もできないため、定量的な選定根拠としては不十分です。人間評価ワークフロー（C）は品質評価に優れていますが、20名の保険部門社員の工数確保が必要で、モデル選定段階としてはコストと時間がかかりすぎます。まず自動メトリクスで候補を絞り込むべきです。公式ベンチマーク（D）は一般的な言語能力の指標ですが、日本語の保険約款という特定ドメインでの性能を反映しておらず、自社データでの評価なしに選定するのは不適切です。（スキル1.2.1）",
  "optionExplanations": {
    "A": {
      "correct": true,
      "text": "正解です。自社テストデータセットによるモデル評価ジョブは、保険約款Q&Aという特定ドメインでの正確性を定量的に測定できます。CloudWatch メトリクスとコスト試算を組み合わせることで、日本語対応・3秒以内のレイテンシー・月間 $3,000 以内のコストという3つの要件を客観的に評価し、最適なモデルを選定できます。"
    },
    "B": {
      "correct": false,
      "text": "不正解です。プレイグラウンドでの検証は初期段階の感触確認には有用ですが、10件のテストでは統計的有意性が不十分です。保険約款は生命保険・損害保険・医療保険など多様なカテゴリがあり、少数のテストでは各カテゴリの回答品質を網羅できません。"
    },
    "C": {
      "correct": false,
      "text": "不正解です。人間評価は品質の主観的な側面を捉えるのに優れていますが、モデル選定段階で20名の社員工数を確保するのは効率的ではありません。まず自動メトリクスで候補を絞り込み、最終候補に対して人間評価を行うのが適切な順序です。"
    },
    "D": {
      "correct": false,
      "text": "不正解です。MMLU や HellaSwag は英語中心の一般的な言語理解ベンチマークであり、日本語の保険約款という特定ドメインでの性能を正確に反映しません。プロンプトテンプレートの設定は選定後の最適化であり、モデルの基本的な日本語能力の差を補うものではありません。"
    }
  },
  "references": [
    {
      "title": "Amazon Bedrock モデル評価",
      "url": "https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation.html"
    },
    {
      "title": "Amazon Bedrock モデルの使用",
      "url": "https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html"
    }
  ]
});
